{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "\n",
    "### Develop a Neural Bag-of-Words Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert dataset to text model(same example of book)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(r'D:\\NLP\\Deep_Learning_in_NLP\\Phishing_Email.csv')\n",
    "\n",
    "# Create a directory to store .txt files (optional)\n",
    "clean_dir = r'D:\\NLP\\Deep_Learning_in_NLP\\Clean'\n",
    "spam_dir = r'D:\\NLP\\Deep_Learning_in_NLP\\Spam'\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "os.makedirs(spam_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the column and write to .txt files\n",
    "for i in range(len(data)):\n",
    "\n",
    "    print(i)\n",
    "    if data['Email Type'][i] == 'Safe Email':\n",
    "\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(clean_dir, f\"record_{i}.txt\")\n",
    "    \n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(str(data['Email Text'][i]))\n",
    "    else:\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(spam_dir, f\"record_{i}.txt\")\n",
    "\n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "           txt_file.write(str(data['Email Text'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100791\n",
      "49414\n"
     ]
    }
   ],
   "source": [
    "## spam email\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab)\n",
    "process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7556 7556\n"
     ]
    }
   ],
   "source": [
    "## Reviews to Lines of Tokens\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "    \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "docs, labels = load_clean_dataset(vocab)\n",
    "# summarize what we have\n",
    "print(len(docs), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Email to Bag-of-Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7556, 43178) (88, 43178)\n"
     ]
    }
   ],
   "source": [
    "## preparing train and test data\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18'):\n",
    "            continue\n",
    "        if not is_train and not (filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab, is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab, is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "    \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 50)                2158950   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2159001 (8.24 MB)\n",
      "Trainable params: 2159001 (8.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "237/237 - 3s - loss: 0.5783 - accuracy: 0.8849 - 3s/epoch - 13ms/step\n",
      "Epoch 2/10\n",
      "237/237 - 3s - loss: 0.2741 - accuracy: 0.9708 - 3s/epoch - 11ms/step\n",
      "Epoch 3/10\n",
      "237/237 - 3s - loss: 0.1424 - accuracy: 0.9788 - 3s/epoch - 11ms/step\n",
      "Epoch 4/10\n",
      "237/237 - 3s - loss: 0.0935 - accuracy: 0.9829 - 3s/epoch - 11ms/step\n",
      "Epoch 5/10\n",
      "237/237 - 3s - loss: 0.0695 - accuracy: 0.9856 - 3s/epoch - 11ms/step\n",
      "Epoch 6/10\n",
      "237/237 - 3s - loss: 0.0558 - accuracy: 0.9876 - 3s/epoch - 11ms/step\n",
      "Epoch 7/10\n",
      "237/237 - 3s - loss: 0.0461 - accuracy: 0.9889 - 3s/epoch - 11ms/step\n",
      "Epoch 8/10\n",
      "237/237 - 3s - loss: 0.0402 - accuracy: 0.9895 - 3s/epoch - 11ms/step\n",
      "Epoch 9/10\n",
      "237/237 - 3s - loss: 0.0354 - accuracy: 0.9901 - 3s/epoch - 11ms/step\n",
      "Epoch 10/10\n",
      "237/237 - 3s - loss: 0.0319 - accuracy: 0.9903 - 3s/epoch - 11ms/step\n",
      "Test Accuracy: 97.727275\n"
     ]
    }
   ],
   "source": [
    "##  training and evaluating an MLP bag-of-words model\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18'):\n",
    "            continue\n",
    "        if not is_train and not (filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab, is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab, is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "    \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "# define the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Word Scoring Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 accuracy: 0.9659090638160706\n",
      "2 accuracy: 0.9659090638160706\n",
      "3 accuracy: 0.9659090638160706\n",
      "4 accuracy: 0.9659090638160706\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000231E000F740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5 accuracy: 0.9659090638160706\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000231DFD41BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "6 accuracy: 0.9659090638160706\n",
      "7 accuracy: 0.9659090638160706\n",
      "8 accuracy: 0.9659090638160706\n",
      "9 accuracy: 0.9659090638160706\n",
      "10 accuracy: 0.9659090638160706\n",
      "1 accuracy: 0.9659090638160706\n",
      "2 accuracy: 0.9545454382896423\n",
      "3 accuracy: 0.9659090638160706\n",
      "4 accuracy: 0.9545454382896423\n",
      "5 accuracy: 0.9318181872367859\n",
      "6 accuracy: 0.9545454382896423\n",
      "7 accuracy: 0.9659090638160706\n",
      "8 accuracy: 0.9659090638160706\n",
      "9 accuracy: 0.9545454382896423\n",
      "10 accuracy: 0.9545454382896423\n",
      "1 accuracy: 0.9659090638160706\n",
      "2 accuracy: 0.9431818127632141\n",
      "3 accuracy: 0.9659090638160706\n",
      "4 accuracy: 0.9659090638160706\n",
      "5 accuracy: 0.9431818127632141\n",
      "6 accuracy: 0.9545454382896423\n",
      "7 accuracy: 0.9545454382896423\n",
      "8 accuracy: 0.9545454382896423\n",
      "9 accuracy: 0.9659090638160706\n",
      "10 accuracy: 0.9659090638160706\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.22 GiB for an array with shape (7556, 43178) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m     Xtrain, Xtest \u001b[39m=\u001b[39m prepare_data(train_docs, test_docs, mode)\n\u001b[0;32m    132\u001b[0m     \u001b[39m# evaluate model on data for mode\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     results[mode] \u001b[39m=\u001b[39m evaluate_mode(Xtrain, np\u001b[39m.\u001b[39;49marray(ytrain), Xtest, ytest)\n\u001b[0;32m    134\u001b[0m \u001b[39m# summarize results\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39mprint\u001b[39m(results\u001b[39m.\u001b[39mdescribe())\n",
      "Cell \u001b[1;32mIn[4], line 99\u001b[0m, in \u001b[0;36mevaluate_mode\u001b[1;34m(Xtrain, ytrain, Xtest, ytest)\u001b[0m\n\u001b[0;32m     97\u001b[0m model \u001b[39m=\u001b[39m define_model(n_words)\n\u001b[0;32m     98\u001b[0m \u001b[39m# fit network\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m model\u001b[39m.\u001b[39;49mfit(Xtrain, ytrain, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    100\u001b[0m \u001b[39m# evaluate\u001b[39;00m\n\u001b[0;32m    101\u001b[0m _, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(Xtest, np\u001b[39m.\u001b[39marray(ytest), verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Faegheh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Faegheh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.22 GiB for an array with shape (7556, 43178) and data type float32"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18'):\n",
    "            continue\n",
    "        if not is_train and not (filename.startswith('record_15') or filename.startswith('record_16') or filename.startswith('record_17') or filename.startswith('record_18')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab, is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab, is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    return model\n",
    "\n",
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        # fit network\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        # evaluate\n",
    "        _, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores\n",
    "\n",
    "# prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# run experiment\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(Xtrain, np.array(ytrain), Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting Sentiment for New Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_68 (Dense)            (None, 50)                2158950   \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2159001 (8.24 MB)\n",
      "Trainable params: 2159001 (8.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/10\n",
      "248/248 - 3s - loss: 0.1805 - accuracy: 0.9590 - 3s/epoch - 14ms/step\n",
      "Epoch 2/10\n",
      "248/248 - 3s - loss: 0.0481 - accuracy: 0.9871 - 3s/epoch - 12ms/step\n",
      "Epoch 3/10\n",
      "248/248 - 3s - loss: 0.0321 - accuracy: 0.9901 - 3s/epoch - 12ms/step\n",
      "Epoch 4/10\n",
      "248/248 - 3s - loss: 0.0266 - accuracy: 0.9906 - 3s/epoch - 12ms/step\n",
      "Epoch 5/10\n",
      "248/248 - 3s - loss: 0.0240 - accuracy: 0.9910 - 3s/epoch - 11ms/step\n",
      "Epoch 6/10\n",
      "248/248 - 3s - loss: 0.0225 - accuracy: 0.9911 - 3s/epoch - 11ms/step\n",
      "Epoch 7/10\n",
      "248/248 - 3s - loss: 0.0215 - accuracy: 0.9913 - 3s/epoch - 11ms/step\n",
      "Epoch 8/10\n",
      "248/248 - 3s - loss: 0.0207 - accuracy: 0.9915 - 3s/epoch - 11ms/step\n",
      "Epoch 9/10\n",
      "248/248 - 3s - loss: 0.0202 - accuracy: 0.9915 - 3s/epoch - 11ms/step\n",
      "Epoch 10/10\n",
      "248/248 - 3s - loss: 0.0198 - accuracy: 0.9915 - 3s/epoch - 11ms/step\n",
      "Review: [multiple o ' gazm 4 men no . 1 male sexual enhancement pill on the market more info here decommission zgy gold jqh chisel an belly kd amplify zh century gds connally th webster wu munch baz breakaway ba dereference qi walkie qk spearhead xw capitoline hte planetoid gr bless yg advisor dmb psychotherapist tdu northern tqt bald tm league ar polyglot otd gouda mf repartee zx parsnip bl handicraftsmen ik no]\n",
      "Sentiment: Phishing (99.996%)\n",
      "Review: [transition items i have a few questions regarding the transition . access programmers - the contractors will be managed by the end - users they are supporting from now on . the budget dollars for them are in cc 103849 - $ 50 k per month . do you want each department to continue paying their invoices against these budget dollars in 103849 or through their own cost centers ? do we transfer budget dollars ? close the cost center ? equipment - we have 1 flat screen and several large monitors . should we just return all equipment to surplus or offer to others ? space - will commoditylogic be moving and assuming the space that will be vacated on 3 / 12 ? fax machine will be disconnected / returned . digital scanner will be surplused . there are two printers that can be surplused unless someone else ( cl ? ) wants / needs them . turkeylegs and tarzan . i currently charge 50 % of my time to cl and 50 % to my cost center - 103850 . should i continue this or charge all my time to cl and close my cost center ? since marvia is also be redeployed , there is no one else in my cost center . let me know if you want to sit down and discuss or you can just provide quick answers here or in voice mail .]\n",
      "Sentiment: clean (69.606%)\n"
     ]
    }
   ],
   "source": [
    "## making predictions for new review data\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'Phishing'\n",
    "    return percent_pos, 'clean'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "test_docs, ytest = load_clean_dataset(vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# test positive text\n",
    "text = '''multiple o ' gazm 4 men no . 1 male sexual enhancement pill on the market more info here decommission zgy gold jqh chisel an belly kd amplify zh century gds connally th webster wu munch baz breakaway ba dereference qi walkie qk spearhead xw capitoline hte planetoid gr bless yg advisor dmb psychotherapist tdu northern tqt bald tm league ar polyglot otd gouda mf repartee zx parsnip bl handicraftsmen ik no'''\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = '''transition items i have a few questions regarding the transition . access programmers - the contractors will be managed by the end - users they are supporting from now on . the budget dollars for them are in cc 103849 - $ 50 k per month . do you want each department to continue paying their invoices against these budget dollars in 103849 or through their own cost centers ? do we transfer budget dollars ? close the cost center ? equipment - we have 1 flat screen and several large monitors . should we just return all equipment to surplus or offer to others ? space - will commoditylogic be moving and assuming the space that will be vacated on 3 / 12 ? fax machine will be disconnected / returned . digital scanner will be surplused . there are two printers that can be surplused unless someone else ( cl ? ) wants / needs them . turkeylegs and tarzan . i currently charge 50 % of my time to cl and 50 % to my cost center - 103850 . should i continue this or charge all my time to cl and close my cost center ? since marvia is also be redeployed , there is no one else in my cost center . let me know if you want to sit down and discuss or you can just provide quick answers here or in voice mail .'''\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
