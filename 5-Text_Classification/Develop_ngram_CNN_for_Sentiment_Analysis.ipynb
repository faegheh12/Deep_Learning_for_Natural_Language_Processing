{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "###  Develop an n-gram CNN Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preparation data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text  \\\n",
       "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
       "1           1  the other side of * galicismos * * galicismo *...   \n",
       "2           2  re : equistar deal tickets are you still avail...   \n",
       "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
       "4           4  software at incredibly low prices ( 86 % lower...   \n",
       "\n",
       "       Email Type  \n",
       "0      Safe Email  \n",
       "1      Safe Email  \n",
       "2      Safe Email  \n",
       "3  Phishing Email  \n",
       "4  Phishing Email  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing_Email.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "Email Text    0\n",
      "Email Type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Type\n",
      "Safe Email        11322\n",
      "Phishing Email     7312\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "email_type_counts = df['Email Type'].value_counts()\n",
    "print(email_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Safe_Email = df[df[\"Email Type\"]== \"Safe Email\"]\n",
    "Phishing_Email = df[df[\"Email Type\"]== \"Phishing Email\"]\n",
    "Safe_Email = Safe_Email.sample(Phishing_Email.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13046</td>\n",
       "      <td>re : thursday visit frank , we shall have abou...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5009</td>\n",
       "      <td>\\nhttp://www.nationalreview.com/hanson/hanson0...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2525</td>\n",
       "      <td>\\nI stand corrected --- I'd thought I'd seen a...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10628</td>\n",
       "      <td>\\n  There was a server bug on the backup disco...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14668</td>\n",
       "      <td>organizational changes in support of enron \u0001 ,...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text  Email Type\n",
       "0       13046  re : thursday visit frank , we shall have abou...  Safe Email\n",
       "1        5009  \\nhttp://www.nationalreview.com/hanson/hanson0...  Safe Email\n",
       "2        2525  \\nI stand corrected --- I'd thought I'd seen a...  Safe Email\n",
       "3       10628  \\n  There was a server bug on the backup disco...  Safe Email\n",
       "4       14668  organizational changes in support of enron \u0001 ,...  Safe Email"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data= pd.concat([Safe_Email, Phishing_Email], ignore_index = True)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert the above Data similar to the book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store .txt files\n",
    "clean_dir = r\"D:\\NLP\\Deep_Learning_in_NLP\\Clean\"\n",
    "phishing_dir = r\"D:\\NLP\\Deep_Learning_in_NLP\\Phishing\"\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "os.makedirs(phishing_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the column and write to .txt files\n",
    "for i in range(len(Data)):\n",
    "\n",
    "    #print(i)\n",
    "    if Data['Email Type'][i] == 'Safe Email':\n",
    "\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(clean_dir, f\"record_{Data['Unnamed: 0'][i]}.txt\")\n",
    "    \n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(str(Data['Email Text'][i]))\n",
    "    else:\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(phishing_dir, f\"record_{Data['Unnamed: 0'][i]}.txt\")\n",
    "\n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "           txt_file.write(str(Data['Email Text'][i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_8') or filename.startswith('record_9'):\n",
    "            continue\n",
    "        if not is_train and not(filename.startswith('record_8') or filename.startswith('record_9')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load and clean all Emails\n",
    "train_docs, ytrain = load_clean_dataset(True)\n",
    "test_docs, ytest = load_clean_dataset(False)\n",
    "# save training datasets\n",
    "save_dataset([train_docs, ytrain], 'train.pkl')\n",
    "save_dataset([test_docs, ytest], 'test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop Multichannel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 9785\n",
      "Vocabulary size: 77727\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 9785)]               0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 9785)]               0         []                            \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)        [(None, 9785)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)     (None, 9785, 100)            7772700   ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)     (None, 9785, 100)            7772700   ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)     (None, 9785, 100)            7772700   ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 9782, 32)             12832     ['embedding_6[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 9780, 32)             19232     ['embedding_7[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 9778, 32)             25632     ['embedding_8[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 9782, 32)             0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 9780, 32)             0         ['conv1d_7[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 9778, 32)             0         ['conv1d_8[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 4891, 32)             0         ['dropout_6[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 4890, 32)             0         ['dropout_7[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 4889, 32)             0         ['dropout_8[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)         (None, 156512)               0         ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)         (None, 156480)               0         ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)         (None, 156448)               0         ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 469440)               0         ['flatten_6[0][0]',           \n",
      " )                                                                   'flatten_7[0][0]',           \n",
      "                                                                     'flatten_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 10)                   4694410   ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    11        ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28070217 (107.08 MB)\n",
      "Trainable params: 28070217 (107.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/7\n",
      "383/383 [==============================] - 171s 444ms/step - loss: 0.3266 - accuracy: 0.8171\n",
      "Epoch 2/7\n",
      "383/383 [==============================] - 172s 449ms/step - loss: 0.0435 - accuracy: 0.9853\n",
      "Epoch 3/7\n",
      "383/383 [==============================] - 166s 433ms/step - loss: 0.0263 - accuracy: 0.9900\n",
      "Epoch 4/7\n",
      "383/383 [==============================] - 162s 423ms/step - loss: 0.0197 - accuracy: 0.9912\n",
      "Epoch 5/7\n",
      "383/383 [==============================] - 161s 420ms/step - loss: 0.0214 - accuracy: 0.9910\n",
      "Epoch 6/7\n",
      "383/383 [==============================] - 161s 420ms/step - loss: 0.0225 - accuracy: 0.9912\n",
      "Epoch 7/7\n",
      "383/383 [==============================] - 161s 421ms/step - loss: 0.0287 - accuracy: 0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faegheh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    return model\n",
    "\n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], np.array(trainLabels), epochs=7, batch_size=16)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 9785\n",
      "Vocabulary size: 77727\n",
      "Train Accuracy: 99.17\n",
      "Test Accuracy: 94.82\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate([trainX,trainX,trainX], np.array(trainLabels), verbose=0)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "_, acc = model.evaluate([testX,testX,testX], np.array(testLabels), verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc1de972ae6407b5943d0669d4a15d3ce8b75699f783b7bfa0010fa5406e3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
