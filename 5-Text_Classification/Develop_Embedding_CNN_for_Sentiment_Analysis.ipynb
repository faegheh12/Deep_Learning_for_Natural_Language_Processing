{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Develop an Embedding + CNN Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preparation data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text   \n",
       "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...  \\\n",
       "1           1  the other side of * galicismos * * galicismo *...   \n",
       "2           2  re : equistar deal tickets are you still avail...   \n",
       "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
       "4           4  software at incredibly low prices ( 86 % lower...   \n",
       "\n",
       "       Email Type  \n",
       "0      Safe Email  \n",
       "1      Safe Email  \n",
       "2      Safe Email  \n",
       "3  Phishing Email  \n",
       "4  Phishing Email  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing_Email.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "Email Text    0\n",
      "Email Type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Type\n",
      "Safe Email        11322\n",
      "Phishing Email     7312\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "email_type_counts = df['Email Type'].value_counts()\n",
    "print(email_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Safe_Email = df[df[\"Email Type\"]== \"Safe Email\"]\n",
    "Phishing_Email = df[df[\"Email Type\"]== \"Phishing Email\"]\n",
    "Safe_Email = Safe_Email.sample(Phishing_Email.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17160</td>\n",
       "      <td>On Wed, 28 Aug 2002, Matthew Cline wrote:&gt; The...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11682</td>\n",
       "      <td>empty</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14453</td>\n",
       "      <td>On Mon, 2 Sep 2002, Adam L. Beberg wrote:&gt; Bat...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3395</td>\n",
       "      <td>URL: http://jeremy.zawodny.com/blog/archives/0...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12583</td>\n",
       "      <td>sum : recursos para el espanol ( spanish resou...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text  Email Type\n",
       "0       17160  On Wed, 28 Aug 2002, Matthew Cline wrote:> The...  Safe Email\n",
       "1       11682                                              empty  Safe Email\n",
       "2       14453  On Mon, 2 Sep 2002, Adam L. Beberg wrote:> Bat...  Safe Email\n",
       "3        3395  URL: http://jeremy.zawodny.com/blog/archives/0...  Safe Email\n",
       "4       12583  sum : recursos para el espanol ( spanish resou...  Safe Email"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data= pd.concat([Safe_Email, Phishing_Email], ignore_index = True)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert the above Data similar to the book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store .txt files\n",
    "clean_dir = r\"D:\\NLP\\Deep_Learning_in_NLP\\Clean\"\n",
    "phishing_dir = r\"D:\\NLP\\Deep_Learning_in_NLP\\Phishing\"\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "os.makedirs(phishing_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the column and write to .txt files\n",
    "for i in range(len(Data)):\n",
    "\n",
    "    #print(i)\n",
    "    if Data['Email Type'][i] == 'Safe Email':\n",
    "\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(clean_dir, f\"record_{Data['Unnamed: 0'][i]}.txt\")\n",
    "    \n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(str(Data['Email Text'][i]))\n",
    "    else:\n",
    "        # Define the file name for the .txt file\n",
    "        filename = os.path.join(phishing_dir, f\"record_{Data['Unnamed: 0'][i]}.txt\")\n",
    "\n",
    "        # Write the record to the .txt file\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as txt_file:\n",
    "           txt_file.write(str(Data['Email Text'][i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87350\n",
      "42264\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "############ Loading and Cleaning Emails\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('record_8') or filename.startswith('record_9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n",
    "################### Define a Vocabulary\n",
    "\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab)\n",
    "process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train CNN With Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36730\n",
      "Maximum length: 9683\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 9683, 100)         3673000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 9676, 32)          25632     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 4838, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 154816)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1548170   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5246813 (20.02 MB)\n",
      "Trainable params: 5246813 (20.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/10\n",
      "192/192 - 71s - loss: 0.4931 - accuracy: 0.7771 - 71s/epoch - 372ms/step\n",
      "Epoch 2/10\n",
      "192/192 - 72s - loss: 0.3064 - accuracy: 0.9768 - 72s/epoch - 373ms/step\n",
      "Epoch 3/10\n",
      "192/192 - 71s - loss: 0.2581 - accuracy: 0.9873 - 71s/epoch - 371ms/step\n",
      "Epoch 4/10\n",
      "192/192 - 71s - loss: 0.2257 - accuracy: 0.9879 - 71s/epoch - 367ms/step\n",
      "Epoch 5/10\n",
      "192/192 - 72s - loss: 0.1991 - accuracy: 0.9891 - 72s/epoch - 376ms/step\n",
      "Epoch 6/10\n",
      "192/192 - 73s - loss: 0.1771 - accuracy: 0.9897 - 73s/epoch - 379ms/step\n",
      "Epoch 7/10\n",
      "192/192 - 70s - loss: 0.1588 - accuracy: 0.9899 - 70s/epoch - 367ms/step\n",
      "Epoch 8/10\n",
      "192/192 - 70s - loss: 0.1436 - accuracy: 0.9899 - 70s/epoch - 366ms/step\n",
      "Epoch 9/10\n",
      "192/192 - 71s - loss: 0.1306 - accuracy: 0.9899 - 71s/epoch - 369ms/step\n",
      "Epoch 10/10\n",
      "192/192 - 70s - loss: 0.1199 - accuracy: 0.9895 - 70s/epoch - 367ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faegheh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word    \n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_8') or filename.startswith('record_9'):\n",
    "            continue\n",
    "        if not is_train and not (filename.startswith('record_8') or filename.startswith('record_9')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab, is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab, is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "    \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36730\n",
      "Maximum length: 9683\n",
      "Train Accuracy: 99.00\n",
      "Test Accuracy: 95.16\n",
      "Review: [guaranteed satisfaction , cheapest pr ) escription drug / s on the net !]\n",
      "Sentiment: Phishing (82.319%)\n",
      "Review: [re : 6 . 933 , misc : english only , a footnote on banning of german a brief comment on on banning german in the us midwest during wartime : kurt vonnegut 's semi-autobiographical _ slapstick _ mentions how his german - speaking family self-censored the german out of their speech , music , etc . loren billings billings @ princeton . edu billings @ pucc . bitnet]\n",
      "Sentiment: safe (100.000%)\n"
     ]
    }
   ],
   "source": [
    "## making a prediction on new text data\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin-1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word    \n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('record_8') or filename.startswith('record_9'):\n",
    "            continue\n",
    "        if not is_train and not (filename.startswith('record_8') or filename.startswith('record_9')):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    phishing = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Phishing\", vocab, is_train)\n",
    "    clean = process_docs(r\"D:\\NLP\\Deep_Learning_in_NLP\\phishing_detection\\Clean\", vocab, is_train)\n",
    "    docs = phishing + clean\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(phishing))] + [1 for _ in range(len(clean))]\n",
    "    return docs, labels\n",
    "    \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'Phishing'\n",
    "    return percent_pos, 'safe'\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, np.array(ytrain), verbose=0)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))\n",
    "# test positive text\n",
    "text = '''guaranteed satisfaction , cheapest pr ) escription drug / s on the net !'''\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = '''re : 6 . 933 , misc : english only , a footnote on banning of german a brief comment on on banning german in the us midwest during wartime : kurt vonnegut 's semi-autobiographical _ slapstick _ mentions how his german - speaking family self-censored the german out of their speech , music , etc . loren billings billings @ princeton . edu billings @ pucc . bitnet'''\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59f8e12afad411f932a8ff1fc0b1a60419aff815880eb960834a6471a09c6b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
