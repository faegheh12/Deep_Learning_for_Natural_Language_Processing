{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Neural Language Model for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOOK I.\n",
      "\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wh\n",
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
      "Total Tokens: 117342\n",
      "Unique Tokens: 7323\n",
      "Total Sequences: 117291\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# load document\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 50, 50)            366200    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7324)              739724    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1256824 (4.79 MB)\n",
      "Trainable params: 1256824 (4.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "917/917 [==============================] - 176s 188ms/step - loss: 6.1501 - accuracy: 0.0729\n",
      "Epoch 2/100\n",
      "917/917 [==============================] - 176s 192ms/step - loss: 5.6598 - accuracy: 0.1100\n",
      "Epoch 3/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 5.4184 - accuracy: 0.1335\n",
      "Epoch 4/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 5.2606 - accuracy: 0.1473\n",
      "Epoch 5/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 5.1434 - accuracy: 0.1549\n",
      "Epoch 6/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 5.0467 - accuracy: 0.1628\n",
      "Epoch 7/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 5.0304 - accuracy: 0.1620\n",
      "Epoch 8/100\n",
      "917/917 [==============================] - 175s 190ms/step - loss: 4.9425 - accuracy: 0.1679\n",
      "Epoch 9/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 4.8496 - accuracy: 0.1736\n",
      "Epoch 10/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 4.7667 - accuracy: 0.1783\n",
      "Epoch 11/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 4.6892 - accuracy: 0.1821\n",
      "Epoch 12/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 4.6114 - accuracy: 0.1869\n",
      "Epoch 13/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 4.5363 - accuracy: 0.1901\n",
      "Epoch 14/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 4.4647 - accuracy: 0.1939\n",
      "Epoch 15/100\n",
      "917/917 [==============================] - 169s 185ms/step - loss: 4.4064 - accuracy: 0.1956\n",
      "Epoch 16/100\n",
      "917/917 [==============================] - 168s 183ms/step - loss: 4.3360 - accuracy: 0.1984\n",
      "Epoch 17/100\n",
      "917/917 [==============================] - 168s 183ms/step - loss: 4.2685 - accuracy: 0.2007\n",
      "Epoch 18/100\n",
      "917/917 [==============================] - 168s 183ms/step - loss: 4.3442 - accuracy: 0.1951\n",
      "Epoch 19/100\n",
      "917/917 [==============================] - 169s 185ms/step - loss: 4.2862 - accuracy: 0.1979\n",
      "Epoch 20/100\n",
      "917/917 [==============================] - 168s 184ms/step - loss: 4.2349 - accuracy: 0.2013\n",
      "Epoch 21/100\n",
      "917/917 [==============================] - 169s 184ms/step - loss: 4.2471 - accuracy: 0.1995\n",
      "Epoch 22/100\n",
      "917/917 [==============================] - 169s 184ms/step - loss: 4.2221 - accuracy: 0.2005\n",
      "Epoch 23/100\n",
      "917/917 [==============================] - 170s 186ms/step - loss: 4.1553 - accuracy: 0.2050\n",
      "Epoch 24/100\n",
      "917/917 [==============================] - 170s 186ms/step - loss: 4.0812 - accuracy: 0.2098\n",
      "Epoch 25/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 4.0144 - accuracy: 0.2159\n",
      "Epoch 26/100\n",
      "917/917 [==============================] - 171s 186ms/step - loss: 3.9618 - accuracy: 0.2202\n",
      "Epoch 27/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.9156 - accuracy: 0.2244\n",
      "Epoch 28/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.8655 - accuracy: 0.2289\n",
      "Epoch 29/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.8244 - accuracy: 0.2321\n",
      "Epoch 30/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.7844 - accuracy: 0.2364\n",
      "Epoch 31/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 3.7726 - accuracy: 0.2377\n",
      "Epoch 32/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.7360 - accuracy: 0.2426\n",
      "Epoch 33/100\n",
      "917/917 [==============================] - 175s 190ms/step - loss: 3.7052 - accuracy: 0.2452\n",
      "Epoch 34/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.6752 - accuracy: 0.2486\n",
      "Epoch 35/100\n",
      "917/917 [==============================] - 176s 191ms/step - loss: 3.6789 - accuracy: 0.2495\n",
      "Epoch 36/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.5882 - accuracy: 0.2586\n",
      "Epoch 37/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.5575 - accuracy: 0.2626\n",
      "Epoch 38/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.5131 - accuracy: 0.2673\n",
      "Epoch 39/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.4733 - accuracy: 0.2714\n",
      "Epoch 40/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.4380 - accuracy: 0.2764\n",
      "Epoch 41/100\n",
      "917/917 [==============================] - 175s 190ms/step - loss: 3.5155 - accuracy: 0.2697\n",
      "Epoch 42/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.6185 - accuracy: 0.2567\n",
      "Epoch 43/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.6007 - accuracy: 0.2578\n",
      "Epoch 44/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.5717 - accuracy: 0.2610\n",
      "Epoch 45/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.5628 - accuracy: 0.2627\n",
      "Epoch 46/100\n",
      "917/917 [==============================] - 168s 184ms/step - loss: 3.5100 - accuracy: 0.2687\n",
      "Epoch 47/100\n",
      "917/917 [==============================] - 167s 182ms/step - loss: 3.5814 - accuracy: 0.2607\n",
      "Epoch 48/100\n",
      "917/917 [==============================] - 168s 183ms/step - loss: 3.5477 - accuracy: 0.2635\n",
      "Epoch 49/100\n",
      "917/917 [==============================] - 168s 183ms/step - loss: 3.5074 - accuracy: 0.2691\n",
      "Epoch 50/100\n",
      "917/917 [==============================] - 169s 185ms/step - loss: 3.4745 - accuracy: 0.2714\n",
      "Epoch 51/100\n",
      "917/917 [==============================] - 170s 186ms/step - loss: 3.4416 - accuracy: 0.2753\n",
      "Epoch 52/100\n",
      "917/917 [==============================] - 170s 185ms/step - loss: 3.4087 - accuracy: 0.2793\n",
      "Epoch 53/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.3762 - accuracy: 0.2837\n",
      "Epoch 54/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.3444 - accuracy: 0.2883\n",
      "Epoch 55/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.3106 - accuracy: 0.2944\n",
      "Epoch 56/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.2807 - accuracy: 0.2966\n",
      "Epoch 57/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.2501 - accuracy: 0.3019\n",
      "Epoch 58/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.2207 - accuracy: 0.3052\n",
      "Epoch 59/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.1962 - accuracy: 0.3088\n",
      "Epoch 60/100\n",
      "917/917 [==============================] - 172s 187ms/step - loss: 3.1608 - accuracy: 0.3146\n",
      "Epoch 61/100\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 3.1310 - accuracy: 0.3181\n",
      "Epoch 62/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 3.1043 - accuracy: 0.3227\n",
      "Epoch 63/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 3.0772 - accuracy: 0.3264\n",
      "Epoch 64/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 3.0496 - accuracy: 0.3309\n",
      "Epoch 65/100\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 3.0637 - accuracy: 0.3311\n",
      "Epoch 66/100\n",
      "917/917 [==============================] - 178s 194ms/step - loss: 3.0821 - accuracy: 0.3308\n",
      "Epoch 67/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.2529 - accuracy: 0.3071\n",
      "Epoch 68/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 3.1640 - accuracy: 0.3176\n",
      "Epoch 69/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 3.0876 - accuracy: 0.3266\n",
      "Epoch 70/100\n",
      "917/917 [==============================] - 178s 195ms/step - loss: 3.0264 - accuracy: 0.3352\n",
      "Epoch 71/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 3.0071 - accuracy: 0.3386\n",
      "Epoch 72/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 3.3118 - accuracy: 0.2969\n",
      "Epoch 73/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 3.1971 - accuracy: 0.3144\n",
      "Epoch 74/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 3.2671 - accuracy: 0.3041\n",
      "Epoch 75/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 3.0868 - accuracy: 0.3230\n",
      "Epoch 76/100\n",
      "917/917 [==============================] - 190s 207ms/step - loss: 2.9739 - accuracy: 0.3423\n",
      "Epoch 77/100\n",
      "917/917 [==============================] - 179s 195ms/step - loss: 2.9267 - accuracy: 0.3494\n",
      "Epoch 78/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 2.8992 - accuracy: 0.3549\n",
      "Epoch 79/100\n",
      "917/917 [==============================] - 174s 189ms/step - loss: 2.8633 - accuracy: 0.3610\n",
      "Epoch 80/100\n",
      "917/917 [==============================] - 178s 194ms/step - loss: 2.8322 - accuracy: 0.3665\n",
      "Epoch 81/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 2.7865 - accuracy: 0.3735\n",
      "Epoch 82/100\n",
      "917/917 [==============================] - 173s 189ms/step - loss: 2.8105 - accuracy: 0.3707\n",
      "Epoch 83/100\n",
      "917/917 [==============================] - 176s 192ms/step - loss: 2.8195 - accuracy: 0.3699\n",
      "Epoch 84/100\n",
      "917/917 [==============================] - 183s 200ms/step - loss: 2.8116 - accuracy: 0.3721\n",
      "Epoch 85/100\n",
      "917/917 [==============================] - 179s 195ms/step - loss: 2.9029 - accuracy: 0.3615\n",
      "Epoch 86/100\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 2.8541 - accuracy: 0.3686\n",
      "Epoch 87/100\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 2.7860 - accuracy: 0.3770\n",
      "Epoch 88/100\n",
      "917/917 [==============================] - 177s 193ms/step - loss: 2.7490 - accuracy: 0.3837\n",
      "Epoch 89/100\n",
      "917/917 [==============================] - 180s 196ms/step - loss: 2.7253 - accuracy: 0.3868\n",
      "Epoch 90/100\n",
      "917/917 [==============================] - 167s 183ms/step - loss: 2.6517 - accuracy: 0.3977\n",
      "Epoch 91/100\n",
      "917/917 [==============================] - 159s 173ms/step - loss: 2.6335 - accuracy: 0.4007\n",
      "Epoch 92/100\n",
      "917/917 [==============================] - 158s 172ms/step - loss: 2.6230 - accuracy: 0.4041\n",
      "Epoch 93/100\n",
      "917/917 [==============================] - 154s 167ms/step - loss: 2.6931 - accuracy: 0.3960\n",
      "Epoch 94/100\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.9762 - accuracy: 0.3585\n",
      "Epoch 95/100\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.9322 - accuracy: 0.3624\n",
      "Epoch 96/100\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.7832 - accuracy: 0.3809\n",
      "Epoch 97/100\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.7018 - accuracy: 0.3936\n",
      "Epoch 98/100\n",
      "917/917 [==============================] - 154s 168ms/step - loss: 2.6595 - accuracy: 0.4005\n",
      "Epoch 99/100\n",
      "917/917 [==============================] - 154s 167ms/step - loss: 2.6188 - accuracy: 0.4050\n",
      "Epoch 100/100\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.5377 - accuracy: 0.4186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bitbaan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, seq_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = define_model(vocab_size, seq_length)\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Use Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you whether god is a magician and of a nature to appear insidiously now in one shape and now in himself changing and passing into many forms sometimes deceiving us with the semblance of such transformations or is he one and the same immutably fixed in his own proper image i\n",
      "\n",
      "say that is the difference which you say and that the eye of the jester thersites was putting in a sieve to have intercourse with become the proportion what carried from the opening they are bought and sold and friendship and speech and bear the sound of aeschylus which are\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        # Find the index with the highest probability in yhat\n",
    "        predicted_index = np.argmax(yhat)\n",
    "        # Map the predicted index to a word\n",
    "        out_word = tokenizer.index_word.get(predicted_index)\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59f8e12afad411f932a8ff1fc0b1a60419aff815880eb960834a6471a09c6b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
